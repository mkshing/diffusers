{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texual Inversion \n",
    "count # of trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPTextConfig\n",
    "\n",
    "config = CLIPTextConfig.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\")\n",
    "config.hidden_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Diffusion \n",
    "Count # of trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.attention_processor import CustomDiffusionAttnProcessor\n",
    "\n",
    "attention_class = CustomDiffusionAttnProcessor\n",
    "freeze_model = \"crossattn_kv\"\n",
    "\n",
    "# now we will add new Custom Diffusion weights to the attention layers\n",
    "# It's important to realize here how many attention weights will be added and of which sizes\n",
    "# The sizes of the attention layers consist only of two different variables:\n",
    "# 1) - the \"hidden_size\", which is increased according to `unet.config.block_out_channels`.\n",
    "# 2) - the \"cross attention size\", which is set to `unet.config.cross_attention_dim`.\n",
    "\n",
    "# Let's first see how many attention processors we will have to set.\n",
    "# For Stable Diffusion, it should be equal to:\n",
    "# - down blocks (2x attention layers) * (2x transformer layers) * (3x down blocks) = 12\n",
    "# - mid blocks (2x attention layers) * (1x transformer layers) * (1x mid blocks) = 2\n",
    "# - up blocks (2x attention layers) * (3x transformer layers) * (3x down blocks) = 18\n",
    "# => 32 layers\n",
    "\n",
    "# Only train key, value projection layers if freeze_model = 'crossattn_kv' else train all params in the cross attention layer\n",
    "train_kv = True\n",
    "train_q_out = False if freeze_model == \"crossattn_kv\" else True\n",
    "custom_diffusion_attn_procs = {}\n",
    "\n",
    "st = unet.state_dict()\n",
    "for name, _ in unet.attn_processors.items():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "    layer_name = name.split(\".processor\")[0]\n",
    "    weights = {\n",
    "        \"to_k_custom_diffusion.weight\": st[layer_name + \".to_k.weight\"],\n",
    "        \"to_v_custom_diffusion.weight\": st[layer_name + \".to_v.weight\"],\n",
    "    }\n",
    "    if train_q_out:\n",
    "        weights[\"to_q_custom_diffusion.weight\"] = st[layer_name + \".to_q.weight\"]\n",
    "        weights[\"to_out_custom_diffusion.0.weight\"] = st[layer_name + \".to_out.0.weight\"]\n",
    "        weights[\"to_out_custom_diffusion.0.bias\"] = st[layer_name + \".to_out.0.bias\"]\n",
    "    if cross_attention_dim is not None:\n",
    "        custom_diffusion_attn_procs[name] = attention_class(\n",
    "            train_kv=train_kv,\n",
    "            train_q_out=train_q_out,\n",
    "            hidden_size=hidden_size,\n",
    "            cross_attention_dim=cross_attention_dim,\n",
    "        ).to(unet.device)\n",
    "        custom_diffusion_attn_procs[name].load_state_dict(weights)\n",
    "    else:\n",
    "        custom_diffusion_attn_procs[name] = attention_class(\n",
    "            train_kv=False,\n",
    "            train_q_out=False,\n",
    "            hidden_size=hidden_size,\n",
    "            cross_attention_dim=cross_attention_dim,\n",
    "        )\n",
    "del st\n",
    "unet.set_attn_processor(custom_diffusion_attn_procs)\n",
    "custom_diffusion_layers = AttnProcsLayers(unet.attn_processors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttnProcsLayers has 19.17 M params.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_params = sum(p.numel() for p in custom_diffusion_layers.parameters()) \n",
    "total_params += 768 # token embed\n",
    "print(f\"{custom_diffusion_layers.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
